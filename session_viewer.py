#!/usr/bin/env python3
"""
AI Session Viewer - è·å– Codex, Claude Code çš„ä¼šè¯è®°å½•

æ”¯æŒçš„ AI CLI å·¥å…·:
- Claude Code: ~/.claude/projects/<project>/*.jsonl
- Codex: ~/.codex/sessions/<year>/<month>/<day>/*.jsonl

åŠŸèƒ½:
1. åˆ—å‡ºæ‰€æœ‰ä¼šè¯è®°å½•
2. æŸ¥çœ‹ä¼šè¯è¯¦æƒ…å’Œæ‘˜è¦
3. ç”Ÿæˆæ¢å¤å¯¹è¯çš„å‘½ä»¤
4. ä½¿ç”¨ LLM æ™ºèƒ½æ€»ç»“ä¼šè¯å†…å®¹
5. æ”¯æŒæœç´¢ä¸é¡¹ç›®/æ—¶é—´è¿‡æ»¤
"""

import json
import os
import sys
import hashlib
import sqlite3
import unicodedata
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import argparse


# ============================================================
# LLM æ€»ç»“å™¨é…ç½®
# ============================================================

@dataclass
class LLMConfig:
    """LLM é…ç½®"""
    provider: str = "openai"      # openai, anthropic, google
    model: str = ""               # æ¨¡å‹åç§°ï¼Œä¸ºç©ºåˆ™ä½¿ç”¨é»˜è®¤
    api_key: str = ""             # API Key
    base_url: str = ""            # è‡ªå®šä¹‰ API åœ°å€
    max_tokens: int = 200         # æœ€å¤§è¾“å‡º token

    # å„æä¾›å•†çš„é»˜è®¤æ¨¡å‹
    DEFAULT_MODELS = {
        "openai": "gpt-4o-mini",
        "anthropic": "claude-3-5-haiku-latest",
        "google": "gemini-2.0-flash",
    }

    def get_model(self) -> str:
        """è·å–æ¨¡å‹åç§°"""
        if self.model:
            return self.model
        return self.DEFAULT_MODELS.get(self.provider, "gpt-4o-mini")

    def get_api_key(self) -> str:
        """è·å– API Keyï¼ˆä¼˜å…ˆä½¿ç”¨é…ç½®ï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰"""
        if self.api_key:
            return self.api_key

        env_keys = {
            "openai": "OPENAI_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
            "google": "GOOGLE_API_KEY",
        }
        env_key = env_keys.get(self.provider, "OPENAI_API_KEY")
        return os.environ.get(env_key, "")


class LLMSummarizer:
    """LLM ä¼šè¯æ€»ç»“å™¨"""

    SUMMARY_PROMPT = """è¯·ç”¨ç®€æ´çš„ä¸­æ–‡æ€»ç»“ä»¥ä¸‹ AI åŠ©æ‰‹ä¼šè¯çš„ä¸»è¦å†…å®¹ã€‚
è¦æ±‚ï¼š
1. ç”¨ 1-3 å¥è¯æ¦‚æ‹¬ä¼šè¯çš„æ ¸å¿ƒä»»åŠ¡æˆ–è®¨è®ºä¸»é¢˜
2. æå–å…³é”®æŠ€æœ¯ç‚¹æˆ–æ“ä½œï¼ˆå¦‚æœ‰ï¼‰
3. æ€»ç»“å­—æ•°æ§åˆ¶åœ¨ 100 å­—ä»¥å†…

ç”¨æˆ·æ¶ˆæ¯åˆ—è¡¨ï¼š
{messages}

è¯·ç›´æ¥è¾“å‡ºæ€»ç»“ï¼Œä¸è¦æœ‰ä»»ä½•å‰ç¼€ã€‚"""

    def __init__(self, config: LLMConfig):
        self.config = config
        self.cache_dir = Path.home() / ".cache" / "ai-session-viewer"
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _get_cache_key(self, messages: list) -> str:
        """ç”Ÿæˆç¼“å­˜ key"""
        content = json.dumps(messages, ensure_ascii=False)
        return hashlib.md5(content.encode()).hexdigest()

    def _get_cached_summary(self, cache_key: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„æ€»ç»“"""
        cache_file = self.cache_dir / f"{cache_key}.txt"
        if cache_file.exists():
            return cache_file.read_text(encoding="utf-8")
        return None

    def _save_cache(self, cache_key: str, summary: str):
        """ä¿å­˜æ€»ç»“åˆ°ç¼“å­˜"""
        cache_file = self.cache_dir / f"{cache_key}.txt"
        cache_file.write_text(summary, encoding="utf-8")

    def summarize(self, messages: list) -> str:
        """ä½¿ç”¨ LLM æ€»ç»“ä¼šè¯"""
        if not messages:
            return "(æ— ç”¨æˆ·æ¶ˆæ¯)"

        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(messages)
        cached = self._get_cached_summary(cache_key)
        if cached:
            return cached

        # å‡†å¤‡æ¶ˆæ¯å†…å®¹ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
        msg_text = "\n".join([f"- {m[:200]}" for m in messages[:10]])
        prompt = self.SUMMARY_PROMPT.format(messages=msg_text)

        try:
            summary = self._call_llm(prompt)
            if summary:
                self._save_cache(cache_key, summary)
                return summary
        except Exception as e:
            return f"(LLM æ€»ç»“å¤±è´¥: {str(e)[:50]})"

        return "(LLM æ€»ç»“å¤±è´¥)"

    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨ LLM API"""
        provider = self.config.provider.lower()

        if provider == "openai":
            return self._call_openai(prompt)
        elif provider == "anthropic":
            return self._call_anthropic(prompt)
        elif provider == "google":
            return self._call_google(prompt)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ LLM æä¾›å•†: {provider}")

    def _call_openai(self, prompt: str) -> str:
        """è°ƒç”¨ OpenAI API"""
        try:
            from openai import OpenAI
        except ImportError:
            raise ImportError("è¯·å®‰è£… openai: pip install openai")

        api_key = self.config.get_api_key()
        if not api_key:
            raise ValueError("æœªè®¾ç½® OPENAI_API_KEY")

        client = OpenAI(
            api_key=api_key,
            base_url=self.config.base_url or None
        )

        response = client.chat.completions.create(
            model=self.config.get_model(),
            messages=[{"role": "user", "content": prompt}],
            max_tokens=self.config.max_tokens,
            temperature=0.3
        )

        return response.choices[0].message.content.strip()

    def _call_anthropic(self, prompt: str) -> str:
        """è°ƒç”¨ Anthropic API"""
        try:
            import anthropic
        except ImportError:
            raise ImportError("è¯·å®‰è£… anthropic: pip install anthropic")

        api_key = self.config.get_api_key()
        if not api_key:
            raise ValueError("æœªè®¾ç½® ANTHROPIC_API_KEY")

        client = anthropic.Anthropic(api_key=api_key)

        response = client.messages.create(
            model=self.config.get_model(),
            max_tokens=self.config.max_tokens,
            messages=[{"role": "user", "content": prompt}]
        )

        return response.content[0].text.strip()

    def _call_google(self, prompt: str) -> str:
        """è°ƒç”¨ Google Gemini API"""
        try:
            import google.generativeai as genai
        except ImportError:
            raise ImportError("è¯·å®‰è£… google-generativeai: pip install google-generativeai")

        api_key = self.config.get_api_key()
        if not api_key:
            raise ValueError("æœªè®¾ç½® GOOGLE_API_KEY")

        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(self.config.get_model())

        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=self.config.max_tokens,
                temperature=0.3
            )
        )

        return response.text.strip()


# å…¨å±€ LLM æ€»ç»“å™¨å®ä¾‹
_llm_summarizer: Optional[LLMSummarizer] = None


def get_llm_summarizer() -> Optional[LLMSummarizer]:
    """è·å–å…¨å±€ LLM æ€»ç»“å™¨"""
    return _llm_summarizer


def set_llm_summarizer(config: LLMConfig):
    """è®¾ç½®å…¨å±€ LLM æ€»ç»“å™¨"""
    global _llm_summarizer
    _llm_summarizer = LLMSummarizer(config)


@dataclass
class SessionInfo:
    """ä¼šè¯ä¿¡æ¯æ•°æ®ç±»"""
    tool: str                    # å·¥å…·åç§°: claude, codex
    session_id: str              # ä¼šè¯ ID
    project_path: str            # é¡¹ç›®è·¯å¾„
    start_time: Optional[datetime] = None  # å¼€å§‹æ—¶é—´
    last_time: Optional[datetime] = None   # æœ€åæ´»åŠ¨æ—¶é—´
    message_count: int = 0       # æ¶ˆæ¯æ•°é‡
    first_message: str = ""      # ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼ˆç”¨äºæ‘˜è¦ï¼‰
    file_path: str = ""          # ä¼šè¯æ–‡ä»¶è·¯å¾„
    file_size: int = 0           # æ–‡ä»¶å¤§å°
    model: str = ""              # ä½¿ç”¨çš„æ¨¡å‹
    topics: list = field(default_factory=list)  # ä¸»é¢˜å…³é”®è¯
    summary: str = ""            # ä¼šè¯å†…å®¹æ€»ç»“
    user_messages: list = field(default_factory=list)  # æ‰€æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆç”¨äºç”Ÿæˆæ€»ç»“ï¼‰


@dataclass
class SessionFilter:
    """ä¼šè¯è¿‡æ»¤æ¡ä»¶"""
    search: str = ""                       # æœç´¢å…³é”®è¯ï¼ˆåŒ¹é…ç”¨æˆ·+åŠ©æ‰‹æ¶ˆæ¯ï¼‰
    project: str = ""                      # é¡¹ç›®è·¯å¾„å…³é”®è¯
    since: Optional[datetime] = None       # å¼€å§‹æ—¶é—´ä¸‹é™ï¼ˆåŒ…å«ï¼‰
    until: Optional[datetime] = None       # å¼€å§‹æ—¶é—´ä¸Šé™ï¼ˆåŒ…å«ï¼‰

    def has_search(self) -> bool:
        return bool(self.search.strip())

    def has_project(self) -> bool:
        return bool(self.project.strip())

    def has_date_range(self) -> bool:
        return self.since is not None or self.until is not None


def build_search_tokens(query: str) -> list[str]:
    """å°†æœç´¢å…³é”®è¯æ‹†åˆ†ä¸º tokens"""
    if not query:
        return []
    return [token.strip().lower() for token in query.split() if token.strip()]


def build_fts_query(query: str) -> str:
    """æ„å»º FTS æŸ¥è¯¢è¡¨è¾¾å¼"""
    tokens = build_search_tokens(query)
    if not tokens:
        return ""
    return " AND ".join(tokens)


def update_search_hits(tokens: list[str], found: set[str], text: str) -> bool:
    """æ›´æ–°å‘½ä¸­çš„æœç´¢ tokenï¼Œå‘½ä¸­å…¨éƒ¨æ—¶è¿”å› True"""
    if not tokens or not text:
        return False
    lowered = text.lower()
    for token in tokens:
        if token not in found and token in lowered:
            found.add(token)
    return len(found) == len(tokens)


def _count_significant_chars(text: str) -> int:
    """ç»Ÿè®¡æœ‰æ•ˆå­—ç¬¦é•¿åº¦"""
    count = 0
    for ch in text:
        if ch.isspace():
            continue
        category = unicodedata.category(ch)
        if category.startswith(("P", "S")):
            continue
        count += 1
    return count


def is_punctuation_only(text: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦åªåŒ…å«æ ‡ç‚¹æˆ–ç¬¦å·"""
    stripped = text.strip()
    if not stripped:
        return False
    return _count_significant_chars(stripped) == 0


def is_separator_line(text: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºåˆ†éš”çº¿æ–‡æœ¬"""
    stripped = text.strip()
    if not stripped:
        return False
    return all(ch in "â”€=â”-_â€”" for ch in stripped)


def is_short_title(text: str, min_length: int = 3) -> bool:
    """åˆ¤æ–­æ ‡é¢˜æ˜¯å¦è¿‡çŸ­"""
    if not text:
        return True
    return _count_significant_chars(text) < min_length


def _get_local_timezone() -> timezone:
    """è·å–æœ¬åœ°æ—¶åŒº"""
    return datetime.now().astimezone().tzinfo or timezone.utc


def normalize_datetime(
    value: Optional[datetime],
    assume_local: bool = False
) -> Optional[datetime]:
    """ç»Ÿä¸€æ—¶é—´å¯¹æ¯”å£å¾„ä¸ºæ— æ—¶åŒºçš„ UTC"""
    if value is None:
        return None
    if value.tzinfo is None:
        tzinfo = _get_local_timezone() if assume_local else timezone.utc
        value = value.replace(tzinfo=tzinfo)
    return value.astimezone(timezone.utc).replace(tzinfo=None)


def format_datetime(
    value: Optional[datetime],
    assume_local: bool = False
) -> Optional[str]:
    """æ ¼å¼åŒ–æ—¶é—´ä¸º UTC ISO å­—ç¬¦ä¸²"""
    normalized = normalize_datetime(value, assume_local=assume_local)
    if normalized is None:
        return None
    return normalized.isoformat()


def to_local_datetime(value: Optional[datetime]) -> Optional[datetime]:
    """å°†æ—¶é—´è½¬æ¢ä¸ºæœ¬åœ°æ—¶åŒº"""
    if value is None:
        return None
    if value.tzinfo is None:
        value = value.replace(tzinfo=timezone.utc)
    return value.astimezone()


def format_local_datetime(value: Optional[datetime]) -> str:
    """æ ¼å¼åŒ–æ—¶é—´ä¸ºæœ¬åœ°æ˜¾ç¤ºå­—ç¬¦ä¸²"""
    local_value = to_local_datetime(value)
    if local_value is None:
        return "æœªçŸ¥"
    return local_value.strftime("%Y-%m-%d %H:%M")


def format_local_iso(value: Optional[datetime]) -> Optional[str]:
    """æ ¼å¼åŒ–æ—¶é—´ä¸ºæœ¬åœ° ISO å­—ç¬¦ä¸²"""
    local_value = to_local_datetime(value)
    if local_value is None:
        return None
    return local_value.isoformat()


def parse_datetime_input(value: str, end_of_day: bool = False) -> datetime:
    """è§£æå‘½ä»¤è¡Œè¾“å…¥çš„æ—¶é—´"""
    if not value:
        raise ValueError("æ—¶é—´å‚æ•°ä¸èƒ½ä¸ºç©º")
    cleaned = value.strip().replace("T", " ")
    if len(cleaned) == 10:
        parsed = datetime.fromisoformat(cleaned)
        if end_of_day:
            return parsed.replace(hour=23, minute=59, second=59)
        return parsed
    return datetime.fromisoformat(cleaned)


def matches_project_filter(project_path: str, project_query: str) -> bool:
    """åˆ¤æ–­é¡¹ç›®è·¯å¾„æ˜¯å¦åŒ¹é…è¿‡æ»¤æ¡ä»¶"""
    if not project_query:
        return True
    return project_query.lower() in (project_path or "").lower()


def matches_date_range(start_time: Optional[datetime], since: Optional[datetime], until: Optional[datetime]) -> bool:
    """åˆ¤æ–­å¼€å§‹æ—¶é—´æ˜¯å¦åœ¨ç­›é€‰èŒƒå›´å†…"""
    if since is None and until is None:
        return True
    if start_time is None:
        return False
    start_cmp = normalize_datetime(start_time)
    since_cmp = normalize_datetime(since)
    until_cmp = normalize_datetime(until)
    if since_cmp and start_cmp < since_cmp:
        return False
    if until_cmp and start_cmp > until_cmp:
        return False
    return True


def extract_text_from_content(content) -> str:
    """ä»æ¶ˆæ¯å†…å®¹ä¸­æå–æ–‡æœ¬"""
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        parts = []
        for item in content:
            if isinstance(item, dict):
                if item.get("type") == "text" and item.get("text"):
                    parts.append(item.get("text"))
            elif isinstance(item, str):
                parts.append(item)
        return " ".join(parts).strip()
    return ""


def get_default_index_path() -> Path:
    """è·å–é»˜è®¤ç´¢å¼•æ•°æ®åº“è·¯å¾„"""
    return Path.home() / ".cache" / "ai-session-viewer" / "index.db"


class SessionParser(ABC):
    """ä¼šè¯è§£æå™¨åŸºç±»"""

    @abstractmethod
    def get_sessions(self, limit: Optional[int] = 10, session_filter: Optional[SessionFilter] = None) -> list[SessionInfo]:
        """è·å–ä¼šè¯åˆ—è¡¨"""
        pass

    @abstractmethod
    def get_resume_command(self, session: SessionInfo) -> str:
        """è·å–æ¢å¤ä¼šè¯çš„å‘½ä»¤"""
        pass

    @abstractmethod
    def get_tool_key(self) -> str:
        """è·å–å·¥å…·æ ‡è¯†"""
        pass

    @abstractmethod
    def get_tool_name(self) -> str:
        """è·å–å·¥å…·åç§°"""
        pass

    @abstractmethod
    def extract_search_text(self, file_path: str) -> str:
        """æå–ç”¨äºæœç´¢çš„ä¼šè¯æ–‡æœ¬"""
        pass


class ClaudeSessionParser(SessionParser):
    """Claude Code ä¼šè¯è§£æå™¨"""
    _SYSTEM_PREFIXES = (
        "You are a Claude-Mem",
        "You are a specialized",
        "IMPORTANT:",
        "# Claude Code",
        "The user sent the following message",
        "PROGRESS SUMMARY CHECKPOINT",
        "## Progress Update",
        "SessionStart:",
        "UserPromptSubmit hook",
        "Caveat: The messages below",
    )
    _SYSTEM_TAG_PREFIXES = (
        "<observed_from_primary_session>",
        "<what_happened>",
        "<local-command-caveat>",
        "<local-command-stdout>",
        "<local-command-",
        "<command-name>",
        "<system-reminder>",
    )

    def __init__(self):
        self.base_path = Path.home() / ".claude"
        self.projects_path = self.base_path / "projects"
        self.history_path = self.base_path / "history.jsonl"

    def get_tool_name(self) -> str:
        return "Claude Code"

    def get_tool_key(self) -> str:
        return "claude"

    def get_sessions(self, limit: Optional[int] = 10, session_filter: Optional[SessionFilter] = None) -> list[SessionInfo]:
        sessions = []

        if not self.projects_path.exists():
            return sessions

        # éå†æ‰€æœ‰é¡¹ç›®ç›®å½•
        for project_dir in self.projects_path.iterdir():
            if not project_dir.is_dir():
                continue

            # æŸ¥æ‰¾ .jsonl ä¼šè¯æ–‡ä»¶
            for session_file in project_dir.glob("*.jsonl"):
                if session_file.name.startswith("."):
                    continue

                session = self._parse_session_file(session_file, project_dir.name, session_filter)
                # åªæ·»åŠ æœ‰ç”¨æˆ·è¾“å…¥çš„ä¼šè¯
                if session and session.first_message:
                    sessions.append(session)

        # æŒ‰æœ€åæ´»åŠ¨æ—¶é—´æ’åº
        sessions.sort(key=lambda s: s.last_time.replace(tzinfo=None) if s.last_time else datetime.min, reverse=True)
        if limit is None:
            return sessions
        return sessions[:limit]

    def _parse_session_file(self, file_path: Path, project_name: str, session_filter: Optional[SessionFilter]) -> Optional[SessionInfo]:
        """è§£æ Claude ä¼šè¯æ–‡ä»¶"""
        try:
            session_id = file_path.stem
            messages = []
            user_messages = []
            first_user_message = ""
            model = ""
            start_time = None
            last_time = None
            search_tokens = build_search_tokens(session_filter.search) if session_filter and session_filter.has_search() else []
            search_found = set()

            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        data = json.loads(line)
                        msg_type = data.get("type", "")

                        # è·å–æ—¶é—´æˆ³
                        ts = data.get("timestamp")
                        if ts:
                            try:
                                msg_time = datetime.fromisoformat(ts.replace("Z", "+00:00"))
                                if start_time is None:
                                    start_time = msg_time
                                last_time = msg_time
                            except:
                                pass

                        # æå–ç”¨æˆ·æ¶ˆæ¯
                        if msg_type == "user":
                            messages.append(data)
                            msg = data.get("message", {})
                            content = msg.get("content", "")
                            msg_text = extract_text_from_content(content)
                            if msg_text:
                                # åˆ¤æ–­æ˜¯å¦ä¸ºç”¨æˆ·æ‰‹åŠ¨è¾“å…¥ï¼ˆéç³»ç»Ÿæ³¨å…¥ï¼‰
                                is_user_input = self._is_user_manual_input(msg_text)
                                if is_user_input:
                                    user_messages.append(msg_text)
                                    if not first_user_message:
                                        first_user_message = msg_text[:100]
                                    elif is_short_title(first_user_message) and not is_short_title(msg_text):
                                        # å¦‚æœé¦–æ¡æ¶ˆæ¯å¤ªçŸ­ï¼Œç”¨åç»­è¾ƒé•¿æ¶ˆæ¯æ›¿æ¢
                                        first_user_message = msg_text[:100]
                                    if search_tokens:
                                        update_search_hits(search_tokens, search_found, msg_text)

                        # æå–æ¨¡å‹ä¿¡æ¯
                        if msg_type == "assistant":
                            msg = data.get("message", {})
                            if msg.get("model") and not model:
                                model = msg.get("model", "")
                            if search_tokens:
                                content = msg.get("content", "")
                                msg_text = extract_text_from_content(content)
                                if msg_text:
                                    update_search_hits(search_tokens, search_found, msg_text)

                    except json.JSONDecodeError:
                        continue

            # å°†é¡¹ç›®åè½¬æ¢ä¸ºå®é™…è·¯å¾„
            project_path = project_name.replace("-", "/")
            if project_path.startswith("/"):
                project_path = project_path[1:]

            if session_filter and session_filter.has_project():
                if not matches_project_filter(project_path, session_filter.project.strip()):
                    return None

            if session_filter and session_filter.has_date_range():
                if not matches_date_range(start_time, session_filter.since, session_filter.until):
                    return None

            if search_tokens and len(search_found) != len(search_tokens):
                return None

            # ç”Ÿæˆä¼šè¯æ€»ç»“
            summary = self._generate_summary(user_messages)

            return SessionInfo(
                tool="claude",
                session_id=session_id,
                project_path=project_path,
                start_time=start_time,
                last_time=last_time,
                message_count=len(messages),
                first_message=first_user_message,
                file_path=str(file_path),
                file_size=file_path.stat().st_size,
                model=model,
                summary=summary,
                user_messages=user_messages
            )
        except Exception as e:
            return None

    def _is_user_manual_input(self, content: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç”¨æˆ·æ‰‹åŠ¨è¾“å…¥ï¼ˆéç³»ç»Ÿæ³¨å…¥ï¼‰"""
        stripped = content.strip()
        if not stripped:
            return False

        for prefix in self._SYSTEM_PREFIXES:
            if stripped.startswith(prefix):
                return False

        for tag in self._SYSTEM_TAG_PREFIXES:
            if stripped.startswith(tag):
                return False

        if is_separator_line(stripped) or is_punctuation_only(stripped):
            return False

        return True

    def _generate_summary(self, user_messages: list) -> str:
        """æ ¹æ®ç”¨æˆ·æ¶ˆæ¯ç”Ÿæˆä¼šè¯æ€»ç»“"""
        if not user_messages:
            return "(æ— ç”¨æˆ·æ¶ˆæ¯)"

        # å¦‚æœé…ç½®äº† LLM æ€»ç»“å™¨ï¼Œä½¿ç”¨ LLM ç”Ÿæˆæ€»ç»“
        summarizer = get_llm_summarizer()
        if summarizer:
            return summarizer.summarize(user_messages)

        # å¦åˆ™ä½¿ç”¨ç®€å•çš„æ–‡æœ¬æå–
        summary_parts = []
        for msg in user_messages[:5]:
            clean_msg = msg.strip()
            if len(clean_msg) > 60:
                clean_msg = clean_msg[:60] + "..."
            if clean_msg:
                summary_parts.append(f"â€¢ {clean_msg}")

        if len(user_messages) > 5:
            summary_parts.append(f"  ... è¿˜æœ‰ {len(user_messages) - 5} æ¡æ¶ˆæ¯")

        return "\n".join(summary_parts) if summary_parts else "(æ— æœ‰æ•ˆæ¶ˆæ¯)"

    def get_resume_command(self, session: SessionInfo) -> str:
        """ç”Ÿæˆ Claude æ¢å¤å‘½ä»¤"""
        # Claude Code ä½¿ç”¨ -r <session_id> æ¢å¤æŒ‡å®šä¼šè¯
        return f"claude -r {session.session_id}"

    def extract_search_text(self, file_path: str) -> str:
        """æå– Claude ä¼šè¯çš„ç”¨æˆ·+åŠ©æ‰‹æ¶ˆæ¯æ–‡æœ¬"""
        parts = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        data = json.loads(line)
                        msg_type = data.get("type", "")
                        msg = data.get("message", {})
                        content = extract_text_from_content(msg.get("content", ""))

                        if msg_type == "user":
                            if content and self._is_user_manual_input(content):
                                parts.append(content)
                        elif msg_type == "assistant":
                            if content:
                                parts.append(content)
                    except json.JSONDecodeError:
                        continue
        except Exception:
            return ""
        return "\n".join(parts)


class CodexSessionParser(SessionParser):
    """Codex ä¼šè¯è§£æå™¨"""

    def __init__(self):
        self.base_path = Path.home() / ".codex"
        self.sessions_path = self.base_path / "sessions"
        self.history_path = self.base_path / "history.jsonl"

    def get_tool_name(self) -> str:
        return "Codex"

    def get_tool_key(self) -> str:
        return "codex"

    def get_sessions(self, limit: Optional[int] = 10, session_filter: Optional[SessionFilter] = None) -> list[SessionInfo]:
        sessions = []

        # ç„¶åä» sessions ç›®å½•è·å–è¯¦ç»†ä¿¡æ¯
        if self.sessions_path.exists():
            for year_dir in sorted(self.sessions_path.iterdir(), reverse=True):
                if not year_dir.is_dir() or not year_dir.name.isdigit():
                    continue
                for month_dir in sorted(year_dir.iterdir(), reverse=True):
                    if not month_dir.is_dir() or not month_dir.name.isdigit():
                        continue
                    for day_dir in sorted(month_dir.iterdir(), reverse=True):
                        if not day_dir.is_dir() or not day_dir.name.isdigit():
                            continue
                        for session_file in day_dir.glob("*.jsonl"):
                            session = self._parse_session_file(session_file, session_filter)
                            # åªæ·»åŠ æœ‰ç”¨æˆ·è¾“å…¥çš„ä¼šè¯
                            if session and session.first_message:
                                sessions.append(session)
                            if limit is not None and len(sessions) >= limit:
                                break
                        if limit is not None and len(sessions) >= limit:
                            break
                    if limit is not None and len(sessions) >= limit:
                        break
                if limit is not None and len(sessions) >= limit:
                    break

        # æŒ‰æœ€åæ´»åŠ¨æ—¶é—´æ’åº
        sessions.sort(key=lambda s: s.last_time.replace(tzinfo=None) if s.last_time else datetime.min, reverse=True)
        if limit is None:
            return sessions
        return sessions[:limit]

    def _parse_session_file(self, file_path: Path, session_filter: Optional[SessionFilter]) -> Optional[SessionInfo]:
        """è§£æ Codex ä¼šè¯æ–‡ä»¶"""
        try:
            session_id = ""
            cwd = ""
            first_message = ""
            model = ""
            start_time = None
            last_time = None
            message_count = 0
            user_messages = []
            search_tokens = build_search_tokens(session_filter.search) if session_filter and session_filter.has_search() else []
            search_found = set()

            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        data = json.loads(line)
                        msg_type = data.get("type", "")

                        # è·å–ä¼šè¯å…ƒæ•°æ®
                        if msg_type == "session_meta":
                            payload = data.get("payload", {})
                            session_id = payload.get("id", "")
                            cwd = payload.get("cwd", "")
                            ts = data.get("timestamp")
                            if ts:
                                try:
                                    start_time = datetime.fromisoformat(ts.replace("Z", "+00:00"))
                                except:
                                    pass

                        # è·å–ç”¨æˆ·æ¶ˆæ¯
                        if msg_type == "message" and data.get("role") == "user":
                            message_count += 1
                            content = data.get("content", "")
                            msg_text = extract_text_from_content(content)
                            if msg_text and not first_message:
                                first_message = msg_text[:100]

                            if msg_text:
                                user_messages.append(msg_text)
                                if search_tokens:
                                    update_search_hits(search_tokens, search_found, msg_text)

                            ts = data.get("timestamp")
                            if ts:
                                try:
                                    last_time = datetime.fromisoformat(ts.replace("Z", "+00:00"))
                                except:
                                    pass

                        # è·å–æ¨¡å‹ä¿¡æ¯
                        if msg_type == "message" and data.get("role") == "assistant":
                            if data.get("model") and not model:
                                model = data.get("model", "")
                            if search_tokens:
                                content = data.get("content", "")
                                msg_text = extract_text_from_content(content)
                                if msg_text:
                                    update_search_hits(search_tokens, search_found, msg_text)

                    except json.JSONDecodeError:
                        continue

            if not session_id:
                # ä»æ–‡ä»¶åæå– session_id
                session_id = file_path.stem.split("-")[-1] if "-" in file_path.stem else file_path.stem

            if session_filter and session_filter.has_project():
                if not matches_project_filter(cwd, session_filter.project.strip()):
                    return None

            if session_filter and session_filter.has_date_range():
                if not matches_date_range(start_time, session_filter.since, session_filter.until):
                    return None

            if search_tokens and len(search_found) != len(search_tokens):
                return None

            # ç”Ÿæˆä¼šè¯æ€»ç»“
            summary = self._generate_summary(user_messages)

            return SessionInfo(
                tool="codex",
                session_id=session_id,
                project_path=cwd,
                start_time=start_time,
                last_time=last_time or start_time,
                message_count=message_count,
                first_message=first_message,
                file_path=str(file_path),
                file_size=file_path.stat().st_size,
                model=model,
                summary=summary,
                user_messages=user_messages
            )
        except Exception as e:
            return None

    def _generate_summary(self, user_messages: list) -> str:
        """æ ¹æ®ç”¨æˆ·æ¶ˆæ¯ç”Ÿæˆä¼šè¯æ€»ç»“"""
        if not user_messages:
            return "(æ— ç”¨æˆ·æ¶ˆæ¯)"

        summary_parts = []
        for msg in user_messages[:5]:
            clean_msg = msg.strip()
            if len(clean_msg) > 60:
                clean_msg = clean_msg[:60] + "..."
            if clean_msg:
                summary_parts.append(f"â€¢ {clean_msg}")

        if len(user_messages) > 5:
            summary_parts.append(f"  ... è¿˜æœ‰ {len(user_messages) - 5} æ¡æ¶ˆæ¯")

        return "\n".join(summary_parts) if summary_parts else "(æ— æœ‰æ•ˆæ¶ˆæ¯)"

    def get_resume_command(self, session: SessionInfo) -> str:
        """ç”Ÿæˆ Codex æ¢å¤å‘½ä»¤"""
        return f"codex --resume {session.session_id}"

    def extract_search_text(self, file_path: str) -> str:
        """æå– Codex ä¼šè¯çš„ç”¨æˆ·+åŠ©æ‰‹æ¶ˆæ¯æ–‡æœ¬"""
        parts = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        data = json.loads(line)
                        if data.get("type") != "message":
                            continue
                        role = data.get("role", "")
                        if role not in ("user", "assistant"):
                            continue
                        content = extract_text_from_content(data.get("content", ""))
                        if content:
                            parts.append(content)
                    except json.JSONDecodeError:
                        continue
        except Exception:
            return ""
        return "\n".join(parts)


class SessionIndexer:
    """ä¼šè¯ç´¢å¼•å™¨ï¼ˆSQLite + FTS5ï¼‰"""

    def __init__(self, db_path: Path, parsers: list[SessionParser]):
        self.db_path = Path(db_path).expanduser()
        self.parsers = {parser.get_tool_key(): parser for parser in parsers}

    def _ensure_schema(self, conn: sqlite3.Connection):
        """åˆå§‹åŒ–æ•°æ®åº“ç»“æ„"""
        conn.execute("""
            CREATE TABLE IF NOT EXISTS sessions (
                id INTEGER PRIMARY KEY,
                tool TEXT NOT NULL,
                session_id TEXT NOT NULL,
                project_path TEXT,
                start_time TEXT,
                last_time TEXT,
                message_count INTEGER,
                first_message TEXT,
                summary TEXT,
                file_path TEXT NOT NULL UNIQUE,
                file_size INTEGER,
                model TEXT,
                mtime INTEGER
            )
        """)
        conn.execute("CREATE INDEX IF NOT EXISTS idx_sessions_tool ON sessions(tool)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_sessions_start_time ON sessions(start_time)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_sessions_project_path ON sessions(project_path)")
        conn.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS sessions_fts
            USING fts5(content, project_path, session_id, tool)
        """)

    def _load_existing(self, conn: sqlite3.Connection) -> dict[str, dict]:
        """åŠ è½½å·²æœ‰ç´¢å¼•è®°å½•"""
        conn.row_factory = sqlite3.Row
        rows = conn.execute("SELECT id, file_path, file_size, mtime FROM sessions").fetchall()
        existing = {}
        for row in rows:
            existing[row["file_path"]] = {
                "id": row["id"],
                "file_size": row["file_size"],
                "mtime": row["mtime"],
            }
        return existing

    def build_index(self, sessions_by_tool: dict[str, list[SessionInfo]]) -> dict[str, int]:
        """æ„å»ºæˆ–å¢é‡æ›´æ–°ç´¢å¼•"""
        all_sessions = []
        for tool_sessions in sessions_by_tool.values():
            all_sessions.extend(tool_sessions)

        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        stats = {
            "scanned": len(all_sessions),
            "indexed": 0,
            "skipped": 0,
            "removed": 0,
            "errors": 0,
        }

        conn = sqlite3.connect(self.db_path)
        conn.execute("PRAGMA journal_mode=WAL")
        self._ensure_schema(conn)
        existing = self._load_existing(conn)
        seen_paths = set()

        with conn:
            for session in all_sessions:
                file_path = session.file_path
                if not file_path:
                    stats["errors"] += 1
                    continue
                seen_paths.add(file_path)

                file_stat = None
                try:
                    file_stat = Path(file_path).stat()
                except FileNotFoundError:
                    stats["errors"] += 1
                    continue

                current_size = file_stat.st_size
                current_mtime = int(file_stat.st_mtime)
                existing_entry = existing.get(file_path)

                if existing_entry and existing_entry["file_size"] == current_size and existing_entry["mtime"] == current_mtime:
                    stats["skipped"] += 1
                    continue

                parser = self.parsers.get(session.tool)
                if not parser:
                    stats["errors"] += 1
                    continue

                search_text = parser.extract_search_text(file_path)
                start_time = format_datetime(session.start_time)
                last_time = format_datetime(session.last_time)
                project_path = session.project_path or ""
                first_message = session.first_message or ""
                summary = session.summary or ""

                if existing_entry:
                    session_id = existing_entry["id"]
                    conn.execute("""
                        UPDATE sessions
                        SET tool = ?, session_id = ?, project_path = ?, start_time = ?, last_time = ?,
                            message_count = ?, first_message = ?, summary = ?, file_path = ?, file_size = ?,
                            model = ?, mtime = ?
                        WHERE id = ?
                    """, (
                        session.tool,
                        session.session_id,
                        project_path,
                        start_time,
                        last_time,
                        session.message_count,
                        first_message,
                        summary,
                        file_path,
                        current_size,
                        session.model,
                        current_mtime,
                        session_id,
                    ))
                else:
                    cursor = conn.execute("""
                        INSERT INTO sessions (
                            tool, session_id, project_path, start_time, last_time, message_count,
                            first_message, summary, file_path, file_size, model, mtime
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        session.tool,
                        session.session_id,
                        project_path,
                        start_time,
                        last_time,
                        session.message_count,
                        first_message,
                        summary,
                        file_path,
                        current_size,
                        session.model,
                        current_mtime,
                    ))
                    session_id = cursor.lastrowid

                conn.execute("DELETE FROM sessions_fts WHERE rowid = ?", (session_id,))
                conn.execute("""
                    INSERT INTO sessions_fts (rowid, content, project_path, session_id, tool)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    session_id,
                    search_text,
                    project_path,
                    session.session_id,
                    session.tool,
                ))
                stats["indexed"] += 1

            for file_path, entry in existing.items():
                if file_path not in seen_paths:
                    conn.execute("DELETE FROM sessions WHERE id = ?", (entry["id"],))
                    conn.execute("DELETE FROM sessions_fts WHERE rowid = ?", (entry["id"],))
                    stats["removed"] += 1

        conn.close()
        return stats

    def query(self, session_filter: SessionFilter, tool: str, limit: Optional[int]) -> list[SessionInfo]:
        """ä»ç´¢å¼•æ•°æ®åº“æŸ¥è¯¢ä¼šè¯"""
        if not self.db_path.exists():
            return []

        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        clauses = []
        params = []
        join_fts = False

        fts_query = build_fts_query(session_filter.search)
        if fts_query:
            join_fts = True
            clauses.append("sessions_fts MATCH ?")
            params.append(fts_query)

        if session_filter.has_project():
            clauses.append("s.project_path LIKE ?")
            params.append(f"%{session_filter.project.strip()}%")

        if session_filter.has_date_range():
            if session_filter.since:
                clauses.append("s.start_time >= ?")
                params.append(format_datetime(session_filter.since))
            if session_filter.until:
                clauses.append("s.start_time <= ?")
                params.append(format_datetime(session_filter.until))

        if tool != "all":
            clauses.append("s.tool = ?")
            params.append(tool)

        where_clause = f"WHERE {' AND '.join(clauses)}" if clauses else ""
        join_clause = "JOIN sessions_fts ON sessions_fts.rowid = s.id" if join_fts else ""
        limit_clause = "LIMIT ?" if limit is not None else ""
        if limit is not None:
            params.append(limit)

        query = f"""
            SELECT s.*
            FROM sessions s
            {join_clause}
            {where_clause}
            ORDER BY COALESCE(s.last_time, s.start_time) DESC
            {limit_clause}
        """

        rows = conn.execute(query, params).fetchall()
        conn.close()

        sessions = []
        for row in rows:
            start_time = datetime.fromisoformat(row["start_time"]) if row["start_time"] else None
            last_time = datetime.fromisoformat(row["last_time"]) if row["last_time"] else None
            sessions.append(SessionInfo(
                tool=row["tool"],
                session_id=row["session_id"],
                project_path=row["project_path"] or "",
                start_time=start_time,
                last_time=last_time,
                message_count=row["message_count"] or 0,
                first_message=row["first_message"] or "",
                file_path=row["file_path"] or "",
                file_size=row["file_size"] or 0,
                model=row["model"] or "",
                summary=row["summary"] or ""
            ))

        return sessions


class SessionViewer:
    """ä¼šè¯æŸ¥çœ‹å™¨ä¸»ç±»"""

    def __init__(self):
        self.parsers = [
            ClaudeSessionParser(),
            CodexSessionParser()
        ]

    def get_all_sessions(self, limit: Optional[int] = 10, session_filter: Optional[SessionFilter] = None) -> dict[str, list[SessionInfo]]:
        """è·å–æ‰€æœ‰å·¥å…·çš„ä¼šè¯"""
        result = {}
        for parser in self.parsers:
            sessions = parser.get_sessions(limit, session_filter)
            result[parser.get_tool_name()] = sessions
        return result

    def get_sessions_by_tool(self, tool: str, limit: Optional[int] = 10, session_filter: Optional[SessionFilter] = None) -> list[SessionInfo]:
        """è·å–æŒ‡å®šå·¥å…·çš„ä¼šè¯"""
        tool_lower = tool.lower()
        for parser in self.parsers:
            if tool_lower in parser.get_tool_name().lower():
                return parser.get_sessions(limit, session_filter)
        return []

    def get_resume_command(self, session: SessionInfo) -> str:
        """è·å–æ¢å¤å‘½ä»¤"""
        for parser in self.parsers:
            if session.tool in parser.get_tool_name().lower():
                return parser.get_resume_command(session)
        return ""

    def format_session(self, session: SessionInfo, show_detail: bool = False, index: int = 0) -> str:
        """æ ¼å¼åŒ–ä¼šè¯ä¿¡æ¯"""
        lines = []

        # åŸºæœ¬ä¿¡æ¯
        time_str = session.last_time.strftime("%Y-%m-%d %H:%M") if session.last_time else "æœªçŸ¥"
        size_str = self._format_size(session.file_size)

        # ä¼šè¯æ ‡é¢˜ï¼ˆç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰
        title = session.first_message[:60] if session.first_message else "(æ— æ ‡é¢˜)"
        if len(session.first_message) > 60:
            title += "..."

        lines.append(f"ğŸ“Œ [{index}] {title}")
        lines.append(f"   â° {time_str} | ğŸ’¬ {session.message_count} æ¡æ¶ˆæ¯ | ğŸ“Š {size_str}")
        lines.append(f"   ğŸ“ {session.project_path or '(æ— é¡¹ç›®)'}")

        if session.model:
            lines.append(f"   ğŸ¤– {session.model}")

        # æ¢å¤å‘½ä»¤
        resume_cmd = self.get_resume_command(session)
        lines.append(f"   ğŸ”„ {resume_cmd}")

        if show_detail:
            lines.append(f"   ğŸ“„ {session.file_path}")

        return "\n".join(lines)

    def _format_size(self, size: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size < 1024:
            return f"{size} B"
        elif size < 1024 * 1024:
            return f"{size / 1024:.1f} KB"
        else:
            return f"{size / 1024 / 1024:.1f} MB"

    def generate_summary(self, sessions: dict[str, list[SessionInfo]]) -> str:
        """ç”Ÿæˆä¼šè¯æ‘˜è¦"""
        lines = []
        lines.append("=" * 60)
        lines.append("ğŸ” AI ä¼šè¯è®°å½•æ€»ç»“")
        lines.append("=" * 60)

        total_sessions = 0
        total_messages = 0

        for tool_name, tool_sessions in sessions.items():
            count = len(tool_sessions)
            total_sessions += count
            msgs = sum(s.message_count for s in tool_sessions)
            total_messages += msgs

            lines.append(f"\nğŸ“¦ {tool_name}: {count} ä¸ªä¼šè¯, {msgs} æ¡æ¶ˆæ¯")

            if tool_sessions:
                latest = tool_sessions[0]
                time_str = latest.last_time.strftime("%Y-%m-%d %H:%M") if latest.last_time else "æœªçŸ¥"
                lines.append(f"   â””â”€ æœ€è¿‘ä¼šè¯: {time_str}")

        lines.append(f"\nğŸ“Š æ€»è®¡: {total_sessions} ä¸ªä¼šè¯, {total_messages} æ¡æ¶ˆæ¯")
        lines.append("=" * 60)

        return "\n".join(lines)

    def view_session_detail(self, session: SessionInfo):
        """æŸ¥çœ‹ä¼šè¯è¯¦ç»†å†…å®¹"""
        print(f"\n{'=' * 60}")
        print(f"ğŸ“– ä¼šè¯è¯¦æƒ…: {session.session_id[:8]}...")
        print("=" * 60)

        # åŸºæœ¬ä¿¡æ¯
        time_str = session.last_time.strftime("%Y-%m-%d %H:%M") if session.last_time else "æœªçŸ¥"
        start_str = session.start_time.strftime("%Y-%m-%d %H:%M") if session.start_time else "æœªçŸ¥"
        print(f"â° æ—¶é—´: {time_str}")
        print(f"ğŸ•˜ å¼€å§‹: {start_str}")
        print(f"ğŸ“ é¡¹ç›®: {session.project_path or '(æ— )'}")
        print(f"ğŸ¤– æ¨¡å‹: {session.model or '(æœªçŸ¥)'}")
        print(f"ğŸ’¬ æ¶ˆæ¯æ•°: {session.message_count}")
        print(f"ğŸ”„ æ¢å¤å‘½ä»¤: {self.get_resume_command(session)}")
        print(f"{'â”€' * 60}")

        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        if session.user_messages:
            print("\nğŸ“ å¯¹è¯è®°å½• (ç”¨æˆ·æ¶ˆæ¯):\n")
            for i, msg in enumerate(session.user_messages, 1):
                # æ¸…ç†æ¶ˆæ¯å†…å®¹
                clean_msg = msg.strip()
                # è·³è¿‡ç³»ç»Ÿæ¶ˆæ¯
                if clean_msg.startswith("<") or clean_msg.startswith("You are"):
                    continue
                # æˆªæ–­è¿‡é•¿çš„æ¶ˆæ¯
                if len(clean_msg) > 200:
                    clean_msg = clean_msg[:200] + "..."
                print(f"  [{i}] {clean_msg}")
                print()
        else:
            print("\n(æ— ç”¨æˆ·æ¶ˆæ¯è®°å½•)")

        print(f"{'=' * 60}")

    def print_sessions(self, sessions: dict[str, list[SessionInfo]], show_detail: bool = False, interactive: bool = False):
        """æ‰“å°æŒ‡å®šä¼šè¯åˆ—è¡¨"""
        # å…ˆæ‰“å°æ‘˜è¦
        print(self.generate_summary(sessions))

        # æ„å»ºå…¨å±€ä¼šè¯ç´¢å¼•
        all_sessions = []
        session_index = 1

        # æ‰“å°å„å·¥å…·çš„ä¼šè¯è¯¦æƒ…
        for tool_name, tool_sessions in sessions.items():
            print(f"\n{'â”€' * 60}")
            print(f"ğŸ› ï¸  {tool_name} ä¼šè¯åˆ—è¡¨")
            print(f"{'â”€' * 60}")

            if not tool_sessions:
                print("   (æ— ä¼šè¯è®°å½•)")
                continue

            for session in tool_sessions:
                print(f"\n{self.format_session(session, show_detail, session_index)}")
                all_sessions.append(session)
                session_index += 1

        # äº¤äº’å¼æ¨¡å¼
        if interactive and all_sessions:
            print(f"\n{'=' * 60}")
            print("ğŸ’¡ è¾“å…¥åºå·æŸ¥çœ‹ä¼šè¯è¯¦æƒ…ï¼Œè¾“å…¥ q é€€å‡º")
            print("=" * 60)

            while True:
                try:
                    choice = input("\nè¯·é€‰æ‹©ä¼šè¯åºå· (1-{}, q é€€å‡º): ".format(len(all_sessions))).strip()

                    if choice.lower() == 'q':
                        print("ğŸ‘‹ å†è§ï¼")
                        break

                    idx = int(choice)
                    if 1 <= idx <= len(all_sessions):
                        self.view_session_detail(all_sessions[idx - 1])
                    else:
                        print(f"âš ï¸ è¯·è¾“å…¥ 1-{len(all_sessions)} ä¹‹é—´çš„æ•°å­—")
                except ValueError:
                    print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—æˆ– q")
                except KeyboardInterrupt:
                    print("\nğŸ‘‹ å†è§ï¼")
                    break
        else:
            # éäº¤äº’æ¨¡å¼ï¼Œæ‰“å°æ¢å¤å‘½ä»¤ç¤ºä¾‹
            print(f"\n{'=' * 60}")
            print("ğŸ”„ æ¢å¤å¯¹è¯å‘½ä»¤ç¤ºä¾‹")
            print("=" * 60)
            print("""
Claude Code:
  claude -r <session_id>           # æ¢å¤æŒ‡å®šä¼šè¯
  claude --resume                  # æ¢å¤æœ€è¿‘ä¼šè¯

Codex:
  codex --resume <session_id>      # æ¢å¤æŒ‡å®šä¼šè¯
  codex --resume                   # æ¢å¤æœ€è¿‘ä¼šè¯

ğŸ’¡ æç¤º: ä½¿ç”¨ -i å‚æ•°è¿›å…¥äº¤äº’æ¨¡å¼ï¼Œå¯é€‰æ‹©æŸ¥çœ‹ä¼šè¯è¯¦æƒ…
""")

    def print_all(self, limit: int = 5, show_detail: bool = False, interactive: bool = False, session_filter: Optional[SessionFilter] = None):
        """æ‰“å°æ‰€æœ‰ä¼šè¯"""
        sessions = self.get_all_sessions(limit, session_filter)
        self.print_sessions(sessions, show_detail, interactive)


def main():
    parser = argparse.ArgumentParser(
        description="AI ä¼šè¯è®°å½•æŸ¥çœ‹å™¨ - æ”¯æŒ Claude Code, Codex",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s                    # æŸ¥çœ‹æ‰€æœ‰å·¥å…·çš„æœ€è¿‘ 5 ä¸ªä¼šè¯
  %(prog)s -l 10              # æŸ¥çœ‹æœ€è¿‘ 10 ä¸ªä¼šè¯ï¼ˆè¦†ç›–é»˜è®¤å€¼ï¼‰
  %(prog)s -t claude          # åªæŸ¥çœ‹ Claude çš„ä¼šè¯ï¼ˆé»˜è®¤æ˜¾ç¤º 20 ä¸ªï¼‰
  %(prog)s -t claude -l 50    # åªæŸ¥çœ‹ Claude çš„ 50 ä¸ªä¼šè¯
  %(prog)s -d                 # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬æ–‡ä»¶è·¯å¾„ï¼‰
  %(prog)s --summary          # åªæ˜¾ç¤ºæ‘˜è¦
  %(prog)s --search "å…³é”®è¯"  # å…¨å±€æœç´¢ï¼ˆç”¨æˆ·+åŠ©æ‰‹æ¶ˆæ¯ï¼‰
  %(prog)s --project "/path"  # æŒ‰é¡¹ç›®è·¯å¾„ç­›é€‰
  %(prog)s --since 2026-01-01 # æŒ‰å¼€å§‹æ—¶é—´ç­›é€‰
  %(prog)s --build-index      # æ„å»ºç´¢å¼•æ•°æ®åº“ï¼ˆç”¨äº Mac åº”ç”¨ï¼‰

LLM æ™ºèƒ½æ€»ç»“ç¤ºä¾‹:
  %(prog)s --ai-summary                           # ä½¿ç”¨ OpenAI ç”Ÿæˆæ™ºèƒ½æ€»ç»“
  %(prog)s --ai-summary --provider anthropic      # ä½¿ç”¨ Anthropic Claude
  %(prog)s --ai-summary --provider google         # ä½¿ç”¨ Google Gemini
  %(prog)s --ai-summary --model gpt-4o            # æŒ‡å®šæ¨¡å‹

ç¯å¢ƒå˜é‡:
  OPENAI_API_KEY      - OpenAI API Key
  ANTHROPIC_API_KEY   - Anthropic API Key
  GOOGLE_API_KEY      - Google API Key
        """
    )

    parser.add_argument(
        "-l", "--limit",
        type=int,
        default=None,  # æ”¹ä¸ºNoneï¼Œåé¢æ ¹æ®toolç±»å‹æ™ºèƒ½è®¾ç½®
        help="æ¯ä¸ªå·¥å…·æ˜¾ç¤ºçš„ä¼šè¯æ•°é‡ (é»˜è®¤: æŸ¥çœ‹æ‰€æœ‰å·¥å…·æ—¶ä¸º5ï¼Œå•ä¸ªå·¥å…·æ—¶ä¸º20)"
    )

    parser.add_argument(
        "-t", "--tool",
        type=str,
        choices=["claude", "codex", "all"],
        default="all",
        help="æŒ‡å®šæŸ¥çœ‹çš„å·¥å…· (é»˜è®¤: all)"
    )

    parser.add_argument(
        "-d", "--detail",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯"
    )

    parser.add_argument(
        "-i", "--interactive",
        action="store_true",
        help="äº¤äº’æ¨¡å¼ï¼Œå¯é€‰æ‹©æŸ¥çœ‹ä¼šè¯è¯¦æƒ…"
    )

    parser.add_argument(
        "--summary",
        action="store_true",
        help="åªæ˜¾ç¤ºæ‘˜è¦"
    )

    parser.add_argument(
        "--json",
        action="store_true",
        help="ä»¥ JSON æ ¼å¼è¾“å‡º"
    )

    parser.add_argument(
        "--build-index",
        action="store_true",
        help="æ„å»ºç´¢å¼•æ•°æ®åº“ï¼ˆSQLite + FTS5ï¼‰"
    )

    parser.add_argument(
        "--use-index",
        action="store_true",
        help="ä»ç´¢å¼•æ•°æ®åº“è¯»å–ä¼šè¯"
    )

    parser.add_argument(
        "--db-path",
        type=str,
        default="",
        help="ç´¢å¼•æ•°æ®åº“è·¯å¾„ï¼ˆé»˜è®¤: ~/.cache/ai-session-viewer/index.dbï¼‰"
    )

    parser.add_argument(
        "--search",
        type=str,
        default="",
        help="å…¨å±€æœç´¢å…³é”®è¯ï¼ˆåŒ¹é…ç”¨æˆ·+åŠ©æ‰‹æ¶ˆæ¯ï¼‰"
    )

    parser.add_argument(
        "--project",
        type=str,
        default="",
        help="æŒ‰é¡¹ç›®è·¯å¾„å…³é”®è¯ç­›é€‰"
    )

    parser.add_argument(
        "--since",
        type=str,
        default="",
        help="æŒ‰å¼€å§‹æ—¶é—´ç­›é€‰ï¼ˆå¦‚ 2026-01-01 æˆ– 2026-01-01 10:00ï¼‰"
    )

    parser.add_argument(
        "--until",
        type=str,
        default="",
        help="æŒ‰å¼€å§‹æ—¶é—´ç­›é€‰ï¼ˆç»“æŸæ—¶é—´ï¼ŒåŒ…å«ï¼‰"
    )

    # LLM æ€»ç»“ç›¸å…³å‚æ•°
    parser.add_argument(
        "--ai-summary",
        action="store_true",
        help="ä½¿ç”¨ LLM ç”Ÿæˆæ™ºèƒ½ä¼šè¯æ€»ç»“"
    )

    parser.add_argument(
        "--provider",
        type=str,
        choices=["openai", "anthropic", "google"],
        default="openai",
        help="LLM æä¾›å•† (é»˜è®¤: openai)"
    )

    parser.add_argument(
        "--model",
        type=str,
        default="",
        help="æŒ‡å®š LLM æ¨¡å‹åç§° (é»˜è®¤: å„æä¾›å•†çš„é»˜è®¤æ¨¡å‹)"
    )

    parser.add_argument(
        "--api-key",
        type=str,
        default="",
        help="LLM API Key (ä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®)"
    )

    parser.add_argument(
        "--base-url",
        type=str,
        default="",
        help="è‡ªå®šä¹‰ API åœ°å€ (ç”¨äº OpenAI å…¼å®¹æ¥å£)"
    )

    args = parser.parse_args()

    # æ„å»ºè¿‡æ»¤æ¡ä»¶
    since_dt = None
    until_dt = None
    if args.since:
        try:
            since_dt = parse_datetime_input(args.since, end_of_day=False)
        except ValueError as exc:
            print(f"âš ï¸ æ— æ•ˆçš„ --since å‚æ•°: {exc}")
            sys.exit(1)
    if args.until:
        try:
            until_dt = parse_datetime_input(args.until, end_of_day=True)
        except ValueError as exc:
            print(f"âš ï¸ æ— æ•ˆçš„ --until å‚æ•°: {exc}")
            sys.exit(1)
    session_filter = SessionFilter(
        search=args.search,
        project=args.project,
        since=since_dt,
        until=until_dt
    )

    # æ™ºèƒ½è®¾ç½®é»˜è®¤ limitï¼šæœ‰è¿‡æ»¤æ—¶é»˜è®¤ä¸é™åˆ¶
    has_filters = session_filter.has_search() or session_filter.has_project() or session_filter.has_date_range()
    if args.limit is None:
        if args.build_index:
            args.limit = None
        elif has_filters:
            args.limit = None
        else:
            args.limit = 5 if args.tool == "all" else 20

    # å¦‚æœå¯ç”¨äº† AI æ€»ç»“ï¼Œåˆå§‹åŒ– LLM æ€»ç»“å™¨
    if args.ai_summary:
        config = LLMConfig(
            provider=args.provider,
            model=args.model,
            api_key=args.api_key,
            base_url=args.base_url
        )
        set_llm_summarizer(config)
        print(f"ğŸ¤– å·²å¯ç”¨ LLM æ™ºèƒ½æ€»ç»“ (æä¾›å•†: {args.provider}, æ¨¡å‹: {config.get_model()})")
        print()

    viewer = SessionViewer()

    db_path = Path(args.db_path) if args.db_path else get_default_index_path()
    parser_map = {parser.get_tool_key(): parser for parser in viewer.parsers}

    if args.build_index:
        if args.tool == "all":
            sessions = viewer.get_all_sessions(args.limit, session_filter)
        else:
            sessions = {args.tool.capitalize(): viewer.get_sessions_by_tool(args.tool, args.limit, session_filter)}

        indexer = SessionIndexer(db_path, viewer.parsers)
        stats = indexer.build_index(sessions)
        print("ğŸ”§ ç´¢å¼•æ„å»ºå®Œæˆ")
        print(f"ğŸ“¦ æ‰«æä¼šè¯: {stats['scanned']}")
        print(f"âœ… ç´¢å¼•æ›´æ–°: {stats['indexed']}")
        print(f"â­ï¸ è·³è¿‡æœªå˜: {stats['skipped']}")
        print(f"ğŸ§¹ æ¸…ç†ç§»é™¤: {stats['removed']}")
        print(f"âš ï¸ è§£æå¼‚å¸¸: {stats['errors']}")
        print(f"ğŸ“ ç´¢å¼•è·¯å¾„: {db_path}")
        return

    if args.use_index:
        indexer = SessionIndexer(db_path, viewer.parsers)
        results = indexer.query(session_filter, args.tool, args.limit)
        if args.tool == "all":
            sessions = {parser.get_tool_name(): [] for parser in viewer.parsers}
            for session in results:
                parser = parser_map.get(session.tool)
                tool_name = parser.get_tool_name() if parser else session.tool
                sessions.setdefault(tool_name, []).append(session)
        else:
            parser = parser_map.get(args.tool)
            tool_name = parser.get_tool_name() if parser else args.tool
            sessions = {tool_name: results}
    else:
        if args.tool == "all":
            sessions = viewer.get_all_sessions(args.limit, session_filter)
        else:
            sessions = {args.tool.capitalize(): viewer.get_sessions_by_tool(args.tool, args.limit, session_filter)}

    if args.json:
        # JSON è¾“å‡º
        output = {}
        for tool, tool_sessions in sessions.items():
            output[tool] = [
                {
                    "session_id": s.session_id,
                    "project_path": s.project_path,
                    "start_time": s.start_time.isoformat() if s.start_time else None,
                    "last_time": s.last_time.isoformat() if s.last_time else None,
                    "message_count": s.message_count,
                    "first_message": s.first_message,
                    "file_path": s.file_path,
                    "file_size": s.file_size,
                    "model": s.model,
                    "resume_command": viewer.get_resume_command(s)
                }
                for s in tool_sessions
            ]
        print(json.dumps(output, indent=2, ensure_ascii=False))
    elif args.summary:
        print(viewer.generate_summary(sessions))
    else:
        viewer.print_sessions(sessions, args.detail, args.interactive)


if __name__ == "__main__":
    main()
